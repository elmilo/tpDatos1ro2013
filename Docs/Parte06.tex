\section{Métodos de compresión}

Para elegir métodos de compresión para listas de enteros se ha consultado:

\begin{itemize}

\item \citet{Trotman:2003}, (\citeyear{Trotman:2003}): se analizan los métodos Variable Byte (<<vByte>>), Delta, Gamma y Golomb, extrayendo la conclusión que la tasa de compresión es exactamente inversa al listado de los nombres. Golomb obtiene un 50 \% mayor compresión que vByte, mientras que este descomprime 7 veces más rápido que Golomb. Los otros métodos se ubican en el medio de estas cifras, sin alejarse demasiado de Golomb.

\item \citet{Anh:2005}, (\citeyear{Anh:2005}): propone 3 algoritmos de rápida descompresión <<Simple-9>> (S9), <<Relative-10>> y <<Carryover-12>> y los compara con Golomb. Las conclusiones son que Golomb provee entre un 10 \% y un 25\% más compresión que los propuestos, mientras que es hasta un 200 \% más lento para recuperar información.

\item \citet{Zhang:2008}, (\citeyear{Zhang:2008}): analizan S9, vByte, PFor y Rice, y se extraen las conclusiones que Rice es el que mayor tasa de compresión ofrece, muy poco por encima de la compresión óptima, el método de PFor es alrededor de 4 veces rápido al descomprimir apenas ocupando un pequeño porcentaje de almacenamiento de más comparado a Rice.

\item \citet[p.~210]{Buettcher2010}, (\citeyear{Buettcher2010}): analizan Gamma, Delta, Golomb, Rice, vByte y S9. Como es el único que tiene a Golomb y Rice juntos, se extraen conclusiones importantes de aquí. Se puede observar Golomb tiene una ventaja insignificante en cuanto a tasa de compresión, pero es tiene una desventaja importante en cuanto a velocidad de descompresión. En datos duros, es como ganar un 2 \% de espacio, pero perder un 30 \% más de tiempo en descomprimir (la compresión demora lo mismo en ambos).



\end{itemize}

\subsection{Rice coding}

El primer elegido es Rice coding. Conocido hoy en día como un caso particular de Golomb coding, es uno de los métodos más antiguos de compresión, data del año 1979. Se trata de que un entero $n$ es codificado en 2 partes: un cociente $q = \lfloor \frac{n}{2^b} \rfloor$ y un resto 
$r = n \text{ mod } 2^b$. El cociente es almacenado en forma unaria utilizando $q+1$ bits, mientras que el resto es almacenado en formato binario usando $b$ bits. 


%En nuestra implementación, el parámetro $b$ es elegido por bloque, entonces $2^b$ es cercano al valor medio del bloque.

La ventaja principal de este método es que tiene muy buena tasa de compresión. Sin embargo es en general un método lento en términos de velocidad de compresión y descompresión, siendo la causa principal que este método necesita manipular el código unario un bit a la vez, tanto en compresión, como en descompresión. Esto es muy demandante para el CPU, pero, por otro lado, como $b$ es potencia de 2, la compresión y descompresión se hacen lo más eficientemente posible, necesitando operaciones bit a bit, como shifts y máscaras. 

\paragraph{b óptimo}

Golomb comprime con un $b_{opt}= 0.69 \times Promedio$ (promedio de todos los números de la lista a comprimir), y se necesitarán múltiplicaciones y divisiones que enlentecen los procesos de compresión y descompresión. En teoría alcanza mejor tasa de compresión que nuestro elegido Rice, pero en la práctica la diferencia es muy pequeña \cite{Zhang:2008}, \cite{Buettcher2010}.

Para Rice coding, sabemos que $b=2^{\lfloor \log_2{(b_{opt})} \rfloor}$ o $b=2^{\lceil \log_2{(b_{opt})} \rceil}$, y a través de la referencia \cite[p.~200]{Buettcher2010}, encontramos que si llamamos $N$ a la cantidad de documentos, tenemos que:

\[ b_{opt} = \left\lceil \frac{\log_2{(2-\frac{F_T}{N})}}{-\log_2{(1-\frac{F_T}{N})}}  \right\rceil\]


%En nuestra implementación codificaremos una lista de $n$ enteros (con $n$ múltiplo de 32) a la vez. Primero se guardarán todas las partes binarias, utilizando $b$ bits, o sea que en total se ocuparán $n \dot b$ bits para todos. Luego se almacenarán las pares unarias.

\subsection{Frame of reference (FOR)}

(\noindent \textit{Adaptado de \citet[p.~5]{Delbru10adaptiveframe} (\citeyear{Delbru10adaptiveframe})})
\\

El método FOR para una lista de enteros determina el rango de valores posibles, llamado \textit\textbf{<<marco de referencia>>} y mapea cada valor adentro de este rango guardando suficientes bits para que los valores puedan ser distinguibles. 

Por ejemplo, se tiene el siguiente listado (medianamente ordenado y con repetición): 

\[\left< 107,108,110,115,120,125,132,132,131,135 \right> \]

Si utilizáramos 8 bits para almacenar los cada uno de los 10 números, ocuparíamos 80 bits. En cambio, utilizando el punto de vista FOR, veríamos que el rango de número va de 107 a 135, por lo que podríamos restar 107 a todos los números de la secuencia, quedando $\left< 0, 1, 3, 8, 13, 18, 25, 25, 24, 28 \right>$. Ahora cada diferencia puede codificarse con a lo sumo 5 bits. Por supuesto que debe guardarse el mínimo valor utilizando 8 bits, y también indicar con 3 bits más el hecho que solo son 5 bits por valor. En total se ocuparon $(8+3+9*5=45)$ bits, ahorrando un 44 \% con respecto a la opción sin compresión.

Hay un beneficio fundamental de este método relacionado con el guardado de grandes enteros: si quisiéramos el número 25312 y encontramos que el valor mínimo de la lista comprimida es 107 y se usan 5 bits para guardar los elementos, sabremos de entrada que esa lista no tiene el elemento que buscamos.

Dado un marco de referencia $[0, max]$, FOR necesita $\lceil \log_2(max+1) \rceil$ bits, llamados \textit{bits del marco}, para comprimir cada entero en el listado. 

La principal desventaja de FOR es que es sensible a valores atípicos en la lista de enteros. Por ejemplo, si un listado de 1024 enteros contiene 1023 números inferiores a 16 y un solo valor superior a 128, los \textit{bits del marco} serán $\lceil \log_2(128+1) \rceil = 8$, desperdiciando 4 bits por cada otro valor.

Sin embargo la compresión la compresión y descompresión se pueden hacer muy rápidamente, utilizando operaciones de muy poco coste para el CPU. Se utilizan para ello algoritmos con operaciones lógicas bit a bit como AND o SHIFT, sin estructuras de selección que demoran más tiempo en ejecutarse. Esta es la clave de los algoritmos FOR. Se ve un ejemplo en la Tabla \ref{tabla:FOR}.

\begin{table}[!ht]
\centering
\small
\begin{tabular}{l l}
\toprule
\begin{lstlisting}[language=C]
compress3 (int[] i, byte[] b)
{b[0] = (i[0] & 7) 
| ((i[1] & 7) << 3) 
| ((i[2] & 3) << 6); 
b[1] = ((i[2] >> 2) & 1) 
| ((i[3] & 7) << 1) 
| ((i[4] & 7) << 4) 
| ((i[5] & 1) << 7); 
b[2] = ((i[5] >> 1) & 3) 
| ((i[6] & 7) << 2) 
| ((i[7] & 7) << 5);}
\end{lstlisting}
&
\begin{lstlisting}[language=C]
decompress3(byte[] b, int[] i)
{i[0] = (b[0] & 7);
i[1] = (b[0] >> 3) & 7;
i[2] = ((b[1] & 1) << 2)
| (b[0] >> 6);
i[3] = (b[1] >> 1) & 7;
i[4] = (b[1] >> 4) & 7;
i[5] = ((b[2] & 3) << 1)
| (b[1] >> 7);
i[6] = (b[2] >> 2) & 7;
i[7] = (b[2] >> 5) & 7;}
\end{lstlisting}\\
\midrule
Rutina de compresión que codifica 8
&
Rutina de descompresión que decodifica 8 \\
enteros usando 3 bits cada uno 
&
enteros representados por 3 bits cada uno\\
\bottomrule
\end{tabular}
\caption{Compresión y descompresión FOR}\label{tabla:FOR}
\end{table}








%However, compression and decompression is done very efficiently using highly-optimised routines [18] which avoid branching conditions. Each routine is loop-unrolled to encode or decode m values using shift and mask operations only. Listing 1 and 2 show the routines to encode or decode 8 integers with a bit frame of 3. There is a compression and decompression routines for each bit frame.

Dado un listado de $n$ enteros, FOR determina el marco de referencia y codifica el listado a través de pequeñas iteraciones de $m$ enteros usando la misma rutina de compresión en cada iteración. Por cuestiones de rendimiento, $m$ es generalmente elegido múltiplo de 8 para que coincida con la frontera de los bytes.


%In our implementation, FOR relies on routines to encode and decode 32 values at a time.

%La selección de la rutina apropiada para un marco de bits es hecha por una tabla de búsqueda. Al momento de compresión se pasa solo una vez por la lista a comprimir para determinar el marco de bits. Luego se selecciona la rutina asociada al mismo a través de la tabla de búsqueda. Finalmente el marco de bits es almacenado

%The selection of the appropriate routine for a given bit frame is done using a precomputed lookup table. The compression step performs one pass only over the block to determine the bit frame. Then, it selects the routine associated to the bit frame using the lookup table. Finally, the bit frame is stored using one byte in the block header and the compression routine is executed to encode the block. During decompression, FOR reads the bit frame, performs one table lookup to select the decompression routine and executes iteratively the routine over the compressed block.





\paragraph{Listas \textit{delta}}Como la de \textit{d-gaps} presentada anteriormente. Debido a que la distribución de probabilidad generada tomando las diferencias tiende a ser monótonamente decreciente, una práctica común es elegir como marco de referencia $[0, max]$, en donde $max$ es mayor valor entre los \textit{deltas}. 


\subsection{PFOR (\textit{patched-FOR})}

(\noindent \citeauthor{Zukowski:2006}, \citeyear{Zukowski:2006})
\\

El segundo elegido. PFOR es una sencilla modificación al método FOR, con la cual se atenúa el problema de los valores atípicos mencionado anteriormente (aquí denominados \textit{excepciones}). 

%Su idea proviene del principio de Pareto, o regla de 80-20 (\textit{<< el 80 \% de las consecuencias provienen del 20 \% de las causas>>}).

Primero se determina el valor $b$ para el cual la mayoría (digamos, el 80-90 \%) de la lista de enteros a comprimir son menores a $2^b$. Entonces el listado comprimido se guarda en 2 partes: en la primera entran todos los enteros menores a $2^b$ en $b$ bits cada uno, y en la segunda se guardan las \textit{excepciones}, con 8, 16 o 32 bits cada una, dependiendo la excepción más grande.

Al igual que FOR es rápido en la compresión y descompresión, salvo por un pequeño número de elementos, las excepciones. Pero tiene mayor tasa de compresión al no desperdiciar tantos bits en listados en donde los valores son en promedio uniformes salvo por algunos particulares.

\paragraph{PFOR-Delta}

Este método, modificado para \textit{listas delta}, es llamado en la bibliografía (\cite{Zhang:2008} y \cite{Zukowski:2006}), PFOR-Delta.


\subsection{LZW}

Como se verá más adelante, nuestra propuesta para resolver el TP necesita un archivo con palabras guardadas, el Diccionario. 

Para comprimir dicho archivo utilizaremos unos de los métodos clásicos y más simples de compresión, el LZW. Básicamente lo que hace este método es leer una secuencia de símbolos, agruparlos en cadenas de caracteres y convertir las mismas en códigos. Como los códigos ocupan menos lugar que las cadenas de caracteres, se obtiene compresión.

No se hace ningún análisis del texto. En su lugar, solamente agrega cada nueva cadena de caracteres que encuentra a una tabla de que contiene todas las anteriores.

LZW es un algoritmo <<voraz>> (\textit{greedy}) porque trata de encontrar la cadena de caracteres más larga posible para la cual tiene código asignado.


%\begin{lstlisting}[caption=Algoritmo LZW: compresión, label=listing:lzw]
%    STRING = get input character
%    WHILE there are still input characters DO
%        CHARACTER = get input character
%        IF STRING+CHARACTER is in the string table then
%            STRING = STRING+character
%        ELSE
%            output the code for STRING
%            add STRING+CHARACTER to the string table
%            STRING = CHARACTER
%        END of IF
%    END of WHILE
%    output the code for STRING
%\end{lstlisting}
%
%\begin{lstlisting}[caption=Algoritmo LZW: descompresión, label=listing:deslzw]
%Read OLD_CODE
%output OLD_CODE
%WHILE there are still input characters DO
%	Read NEW_CODE
%	STRING = get translation of NEW_CODE
%	output STRING
%	CHARACTER = first character in STRING
%	add OLD_CODE + CHARACTER to the translation table
%	OLD_CODE = NEW_CODE
%END of WHILE
%\end{lstlisting}
